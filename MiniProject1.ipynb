{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'anarchism originated as a term of abuse first used against early working class radicals including th'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"data/training/text8_20m.txt\") as f:\n",
    "    text= f.read()\n",
    "    \n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text Preprocessing** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anarchism',\n",
       " 'originated',\n",
       " 'as',\n",
       " 'a',\n",
       " 'term',\n",
       " 'of',\n",
       " 'abuse',\n",
       " 'first',\n",
       " 'used',\n",
       " 'against']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words= text.split(\" \")\n",
    "\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building the Vocabulary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1336, 2862, 13, 7, 194, 2, 4067, 49, 60, 137]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_count= Counter(words).most_common(60000)\n",
    "\n",
    "most_freq_words= [item for item, _ in words_count]\n",
    "\n",
    "#assigning unique id to every word\n",
    "word_idx= {word: i+1 for i, word in enumerate(most_freq_words)}\n",
    "word_idx['<UNK>']= 0\n",
    "\n",
    "#converting text to idx\n",
    "text_idx= [word_idx.get(w, word_idx['<UNK>']) for w in words]\n",
    "\n",
    "#add freq of unfrequent words\n",
    "words_count.append(('UNK', text_idx.count(0)))\n",
    "\n",
    "text_idx[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1336, 2862),\n",
       " (1336, 13),\n",
       " (2862, 1336),\n",
       " (2862, 13),\n",
       " (2862, 7),\n",
       " (13, 1336),\n",
       " (13, 2862),\n",
       " (13, 7),\n",
       " (13, 194),\n",
       " (7, 2862)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_training_pairs(words, C):\n",
    "    training_pairs= []\n",
    "    n= len(words)\n",
    "    \n",
    "    for i, center_word in enumerate(text_idx):\n",
    "        start_idx= max(0, i-C)\n",
    "        end_idx= min(n-1, i+C+1)\n",
    "        \n",
    "        for j in range(start_idx, end_idx):\n",
    "            if j!=i: #skip center word\n",
    "                training_pairs.append((center_word, words[j]))\n",
    "    \n",
    "    return training_pairs\n",
    "\n",
    "C= 2\n",
    "\n",
    "training_pairs= generate_training_pairs(text_idx, C)\n",
    "\n",
    "training_pairs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate unigram and smoothed unigram distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "N= sum([c for _,c in words_count])\n",
    "alpha= 3/4\n",
    "\n",
    "unigram= {word: freq/N for word,freq in words_count}\n",
    "\n",
    "unigram_sum= sum(u**alpha for u in unigram.values())\n",
    "\n",
    "smoothed_unigram= {word: (uni**alpha)/unigram_sum for word, uni in unigram.items()}\n",
    "\n",
    "unigram_table= np.array(list(smoothed_unigram.values()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from Skipgram import Skipgram\n",
    "from NegativeSamplingLoss import NegativeSamplingLoss\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def train_skipgram(dataset, vocab_size, unigram, embedding_dim, batch_size=64, epochs=5, learning_rate=0.01, num_negatives=20):\n",
    "    \n",
    "    dataloader= DataLoader(dataset, batch_size)\n",
    "    \n",
    "    model= Skipgram(vocab_size, embedding_dim)\n",
    "    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "    model= model.to(device)\n",
    "    \n",
    "    criterion= NegativeSamplingLoss()\n",
    "    optim= Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss= 0\n",
    "        \n",
    "        for center, contexts in dataloader:\n",
    "            center, contexts = center.long(), contexts.long()\n",
    "            center= center.to(device)\n",
    "            contexts= contexts.to(device)\n",
    "            \n",
    "            #make sure batch_size is equal\n",
    "            current_batch_size= len(center)\n",
    "            \n",
    "            #sample negatives from unigram\n",
    "            negatives= torch.tensor(\n",
    "                np.random.choice(\n",
    "                    vocab_size,\n",
    "                    (current_batch_size, num_negatives),\n",
    "                    p= unigram\n",
    "                ),\n",
    "                dtype=torch.long\n",
    "            )\n",
    "            negatives= negatives.to(device)\n",
    "            \n",
    "            #forward pass\n",
    "            positive_score, negative_score= model(center, contexts, negatives)\n",
    "            \n",
    "            loss= criterion(positive_score, negative_score)\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "            total_loss+= loss.item()\n",
    "            \n",
    "        print(f\"Epoch {epoch+1}/{epochs} -- Loss: {total_loss/len(dataloader):.4F}\")\n",
    "    \n",
    "    return model\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 -- Loss: 2.2946\n",
      "Epoch 2/5 -- Loss: 0.0594\n",
      "Epoch 3/5 -- Loss: 0.0114\n",
      "Epoch 4/5 -- Loss: 0.0048\n",
      "Epoch 5/5 -- Loss: 0.0026\n",
      "Time: 131.34 seconds\n"
     ]
    }
   ],
   "source": [
    "from SkipgramDataset import SkipgramDataset\n",
    "import time\n",
    "\n",
    "### Hyperparameters ###\n",
    "EMBEDDING_DIM= 128\n",
    "BATCH_SIZE= 128\n",
    "EPOCHS= 5\n",
    "LEARNIN_RATE= 0.01\n",
    "NUM_NEGATIVES= 10\n",
    "#######################\n",
    "\n",
    "pairs= training_pairs[:100000]\n",
    "dataset= SkipgramDataset(pairs)\n",
    "vocab_size= 60001\n",
    "\n",
    "embedding_dim= 128\n",
    "\n",
    "t0= time.time()\n",
    "model = train_skipgram(dataset, vocab_size, unigram_table,\n",
    "                       embedding_dim= EMBEDDING_DIM,\n",
    "                       batch_size= BATCH_SIZE,\n",
    "                       epochs= EPOCHS,\n",
    "                       learning_rate=LEARNIN_RATE,\n",
    "                       num_negatives=NUM_NEGATIVES)\n",
    "t1= time.time()\n",
    "\n",
    "print(f\"Time: {t1-t0:.2F} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "model.eval()\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "wordsim353_df= pd.read_csv(\"data/evaluation/wordsim353.csv\")\n",
    "\n",
    "wordsim353_df[\"W1 Embeddning\"]= [model.embedding(torch.tensor(word_idx.get(w1)).to(device)).cpu().detach().numpy() if w1 in word_idx.keys() else None\n",
    "                                 for w1 in wordsim353_df['Word 1']]\n",
    "\n",
    "wordsim353_df[\"W2 Embeddning\"]= [model.embedding(torch.tensor(word_idx.get(w2)).to(device)).cpu().detach().numpy() if w2 in word_idx.keys() else None\n",
    "                                 for w2 in wordsim353_df['Word 2']]\n",
    "\n",
    "wordsim353_df['W1 Embeddning'].isna().sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
