{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'anarchism originated as a term of abuse first used against early working class radicals including th'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"data/text8_20m.txt\") as f:\n",
    "    text= f.read()\n",
    "    \n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text Preprocessing** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anarchism',\n",
       " 'originated',\n",
       " 'as',\n",
       " 'a',\n",
       " 'term',\n",
       " 'of',\n",
       " 'abuse',\n",
       " 'first',\n",
       " 'used',\n",
       " 'against']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words= text.split(\" \")\n",
    "\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building the Vocabulary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1336, 2862, 13, 7, 194, 2, 4067, 49, 60, 137]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_count= Counter(words).most_common(60000)\n",
    "\n",
    "most_freq_words= [item for item, _ in words_count]\n",
    "\n",
    "#assigning unique id to every word\n",
    "word_idx= {word: i+1 for i, word in enumerate(most_freq_words)}\n",
    "word_idx['<UNK>']= 0\n",
    "\n",
    "#converting text to idx\n",
    "text_idx= [word_idx.get(w, word_idx['<UNK>']) for w in words]\n",
    "\n",
    "#add freq of unfrequent words\n",
    "words_count.append(('UNK', text_idx.count(0)))\n",
    "\n",
    "text_idx[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1336, 2862),\n",
       " (1336, 13),\n",
       " (2862, 1336),\n",
       " (2862, 13),\n",
       " (2862, 7),\n",
       " (13, 1336),\n",
       " (13, 2862),\n",
       " (13, 7),\n",
       " (13, 194),\n",
       " (7, 2862)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_training_pairs(words, C):\n",
    "    training_pairs= []\n",
    "    n= len(words)\n",
    "    \n",
    "    for i, center_word in enumerate(text_idx):\n",
    "        start_idx= max(0, i-C)\n",
    "        end_idx= min(n-1, i+C+1)\n",
    "        \n",
    "        for j in range(start_idx, end_idx):\n",
    "            if j!=i: #skip center word\n",
    "                training_pairs.append((center_word, words[j]))\n",
    "    \n",
    "    return training_pairs\n",
    "\n",
    "C= 2\n",
    "\n",
    "training_pairs= generate_training_pairs(text_idx, C)\n",
    "\n",
    "training_pairs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate unigram and smoothed unigram distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "N= sum([c for _,c in words_count])\n",
    "alpha= 3/4\n",
    "\n",
    "unigram= {word: freq/N for word,freq in words_count}\n",
    "\n",
    "unigram_sum= sum(u**alpha for u in unigram.values())\n",
    "\n",
    "smoothed_unigram= {word: (uni**alpha)/unigram_sum for word, uni in unigram.items()}\n",
    "\n",
    "unigram_table= np.array(list(smoothed_unigram.values()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from Skipgram import Skipgram\n",
    "from NegativeSamplingLoss import NegativeSamplingLoss\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def train_skipgram(dataset, vocab_size, embedding_dim, unigram, batch_size=32, epochs=5, learning_rate=0.01, num_negatives=20):\n",
    "    \n",
    "    dataloader= DataLoader(dataset, batch_size)\n",
    "    \n",
    "    model= Skipgram(vocab_size, embedding_dim)\n",
    "    criterion= NegativeSamplingLoss()\n",
    "    optim= Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss= 0\n",
    "        \n",
    "        for center, contexts in dataloader:\n",
    "            center, contexts = center.long(), contexts.long()\n",
    "            \n",
    "            #make sure batch_size is equal\n",
    "            current_batch_size= len(center)\n",
    "            \n",
    "            #sample negatives from unigram\n",
    "            negatives= torch.tensor(\n",
    "                np.random.choice(\n",
    "                    vocab_size,\n",
    "                    (current_batch_size, num_negatives),\n",
    "                    p= unigram\n",
    "                ),\n",
    "                dtype=torch.long\n",
    "            )\n",
    "            \n",
    "            #forward pass\n",
    "            positive_score, negative_score= model(center, contexts, negatives)\n",
    "            \n",
    "            loss= criterion(positive_score, negative_score)\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "            total_loss+= loss.item()\n",
    "            \n",
    "        print(f\"Epoch {epoch+1}/{epochs} -- Loss: {total_loss/len(dataloader):.4F}\")\n",
    "    \n",
    "    return model\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing\n",
      "Negative Embedding Shape: torch.Size([32, 20, 128])\n",
      "Center Embedding Shape (Unsqueezed): torch.Size([32, 128, 1])\n",
      "testing\n",
      "Negative Embedding Shape: torch.Size([32, 20, 128])\n",
      "Center Embedding Shape (Unsqueezed): torch.Size([32, 128, 1])\n",
      "testing\n",
      "Negative Embedding Shape: torch.Size([32, 20, 128])\n",
      "Center Embedding Shape (Unsqueezed): torch.Size([32, 128, 1])\n",
      "testing\n",
      "Negative Embedding Shape: torch.Size([32, 20, 128])\n",
      "Center Embedding Shape (Unsqueezed): torch.Size([4, 128, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected size for first two dimensions of batch2 tensor to be: [32, 128] but got: [4, 128].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m vocab_size\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m60001\u001b[39m\n\u001b[1;32m     11\u001b[0m embedding_dim\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[0;32m---> 13\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_skipgram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munigram_table\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 33\u001b[0m, in \u001b[0;36mtrain_skipgram\u001b[0;34m(dataset, vocab_size, embedding_dim, unigram, batch_size, epochs, learning_rate, num_negatives)\u001b[0m\n\u001b[1;32m     23\u001b[0m negatives\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[1;32m     24\u001b[0m     np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\n\u001b[1;32m     25\u001b[0m         vocab_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#forward pass\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m positive_score, negative_score\u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegatives\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m loss\u001b[38;5;241m=\u001b[39m criterion(positive_score, negative_score)\n\u001b[1;32m     37\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/envs/DL3/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DL3/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Uni/Semester 5/Natural Language Processing/Mini_Project1/Skipgram.py:34\u001b[0m, in \u001b[0;36mSkipgram.forward\u001b[0;34m(self, center, context, negatives)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCenter Embedding Shape (Unsqueezed): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcenter_embedding\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#(batch_size x num_neg_samples x embedding_dim) * (batch_size, embedding_dim, 1)\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m negative_score\u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnegatives_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenter_embedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m#(batch_size, num_neg_samples)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (positive_score, negative_score)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected size for first two dimensions of batch2 tensor to be: [32, 128] but got: [4, 128]."
     ]
    }
   ],
   "source": [
    "from SkipgramDataset import SkipgramDataset\n",
    "import importlib\n",
    "\n",
    "\n",
    "\n",
    "pairs= training_pairs[:100]\n",
    "\n",
    "dataset= SkipgramDataset(pairs)\n",
    "\n",
    "vocab_size= 60001\n",
    "embedding_dim= 128\n",
    "\n",
    "model = train_skipgram(dataset, vocab_size, embedding_dim, unigram_table)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
